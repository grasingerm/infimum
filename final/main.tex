\documentclass{article}

\usepackage{mathtools}
\usepackage{bm}
\usepackage[margin=1.25in]{geometry}
\usepackage{courier}
\usepackage{color}
\usepackage{listings}
\usepackage{pdfpages}
\usepackage{enumerate}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}

\newcommand{\blankpage}{
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\newpage
}

\newcommand{\exspace}{\hspace{0.1in}}

\title{Optimization Methods, Final Exam}
\date{2015/12/16}
\author{Matthew Grasinger}

\begin{document}
	
\lstset{language=Matlab,
	keywords={break,case,catch,continue,else,elseif,end,for,function,
		global,if,otherwise,persistent,return,switch,try,while},
	basicstyle=\ttfamily,
	keywordstyle=\color{blue},
	commentstyle=\color{gray},
	stringstyle=\color{dkgreen},
	numbers=left,
	numberstyle=\tiny\color{red},
	stepnumber=1,
	numbersep=10pt,
	backgroundcolor=\color{white},
	tabsize=4,
	showspaces=false,
	showstringspaces=false}
	
\pagenumbering{gobble}
\maketitle
\blankpage
\tableofcontents
\blankpage
\pagenumbering{arabic}

\section{Problem 1} \label{sec:smallest_circle}

\subsection{Objective}

Find the center and radius of the smallest circle that encompasses the following points: $(0,1)$, $(1,0)$, $(1,1)$, $(0,0)$, and whose center lies along the line $y=2x$.

\subsection{Approach 1}

\begin{enumerate}[a)]

\item Express the problem as a constrained minimization problem.

The general equation of a circle can be given as,
\begin{equation} \label{eq:circle_general}
(x - c_x)^2 + (y - c_y)^2 = r^2,
\end{equation}
where $c_x$ and $c_y$ are the $x$ and $y$ coordinates of the center of the circle respectively, and $r$ is the radius of the circle.
The area of a circle is proportional to its radius squared.
Minimizing $r^2$ will be sufficient for finding the smallest circle that encompasses the given points.
By substituting the constraint on the center of the circle into Equation \ref{eq:circle_general}, we obtain the desired constrained minimization formulation,

\begin{align*}
\min_{r, c_x} \exspace & r^2, \\
\text{such that} \exspace & (x_i - c_x)^2 + (y_i - 2c_x)^2 - r^2 \le 0, \\
& \forall i = 1, 2, ..., n.
\end{align*}

\item Find the optimality conditions.

The KKT optimality conditions for the smallest circle problem are,

\begin{eqnarray*}
\boldsymbol{\nabla} f(\mathbf{x}) + \sum_{i}^{n} \mu_i \boldsymbol{\nabla} g_i(\mathbf{x}) &=& \mathbf{0},\\
\begin{bmatrix}2r \\ 0\end{bmatrix} + \sum_{i}^{n} \mu_i \begin{bmatrix}-2r \\ -2(x_i - c_x) - 4(y_i - 2c_x)\end{bmatrix} &=& \mathbf{0},
\end{eqnarray*}

and,

\begin{eqnarray*}
\sum_{i}^{n} \mu_i g_i(\mathbf{x}) &=& 0,\\
\sum_{i}^{n} \mu_i \left[(x_i - c_x)^2 + (y_i - 2c_x)^2 - r^2\right] &=& 0,
\end{eqnarray*}

and lastly,

\begin{equation*}
	\boldsymbol{\mu} \ge \mathbf{0}.
\end{equation*}

%The second order sufficient condition is that

%\begin{eqnarray*}
%	\mathbf{L}(\mathbf{x}) &=& \mathbf{F}(\mathbf{x}) + \boldsymbol{\mu}^T\mathbf{G}(\mathbf{x}),\\
%	&=& \begin{bmatrix}2 & 0\\0 & 0\end{bmatrix} + \boldsymbol{\mu}^T \begin{bmatrix}-2 & 0\\0 & 10\end{bmatrix},
%\end{eqnarray*}

%is a positive definite matrix.

\item Solve the optimality conditions algebraically for the optimal values.

The KKT conditions give the following system of equations,

\begin{eqnarray}	
	\mu_1 + \mu_2 + \mu_3 + \mu_4 &=& 1, \label{eq:kkt1}\\
	10c_x(\mu_1 + \mu_2 + \mu_3 + \mu_4) - 4\mu_1 - 2\mu_2 - 6\mu_3 &=& 0, \label{eq:kkt2}\\
	\mu_1 \left[5c_x^2 - 4c_x + 1 -r^2\right] &=& 0, \label{eq:kkt3}\\
	\mu_2 \left[5c_x^2 - 2c_x + 1 -r^2\right] &=& 0, \label{eq:kkt4}\\
	\mu_3 \left[5c_x^2 - 6c_x + 2 -r^2\right] &=& 0, \label{eq:kkt5}\\
	\mu_4 \left[5c_x^2 -r^2\right] &=& 0, \label{eq:kkt6}\\
	\mu_1 &\ge& 0, \\
	\mu_2 &\ge& 0, \\
	\mu_3 &\ge& 0, \\
	\mu_4 &\ge& 0, \\
	5c_x^2 - 4c_x + 1 -r^2 &\le& 0, \label{eq:ineq1}\\
	5c_x^2 - 2c_x + 1 -r^2 &\le& 0, \label{eq:ineq2}\\
	5c_x^2 - 6c_x + 2 -r^2 &\le& 0, \label{eq:ineq3}\\
	5c_x^2 -r^2 &\le& 0. \label{eq:ineq4}
\end{eqnarray}

In order to solve for the optimal values, we must make assumptions about which constraints are active and which are inactive.
It is clear by inspection of Equation \ref{eq:kkt1} that at least one constraint must be active, and so there is no need to check the possibility of all inactive constraints.
This is consistent with intuition because if there were no active constraints the solution would trivially be a circle with radius zero centered anywhere on the line $y=2x$.

If we assume all of the constraints are active, then the inequalities given by Equations \ref{eq:ineq1}--\ref{eq:ineq4} become equality constraints.
Subtracting Equation \ref{eq:ineq1} from Equation \ref{eq:ineq2} shows that to satisfy both equations, $c_x$ must equal 0.
Plugging $c_x = 0$ into Equation \ref{eq:ineq1} and solving for the radius of the circle yields $r = 1$.
However, plugging $c_x = 0$ into Equation \ref{eq:ineq3} results in, $2 - r^2 \le 0$ which is not true.
This implies the assumption that all constraints are active is not true.
Moreover, this shows that the constraints imposed by the first and second points cannot both be active for solution to be optimal.

Next, let us assume that the constraints imposed by the first point, $(0,1)$, and third point, $(1,1)$, are the only active constraints.
Subtracting Equation \ref{eq:ineq1} from \ref{eq:ineq3} results in $-2c_x + 1 = 0$, or $c_x = 1/2$.
Plugging $c_x = 1/2$ into Equation \ref{eq:ineq1} and solving for the radius of the circle yields $r = 1/2$.
However, $c_x = r = 1/2$ does not satisfy the constraint given by Equation \ref{eq:ineq2}.
This implies the assumption that the constraints imposed by the first and third points are active constraints is not true.

Next, let us assume that the constraints imposed by the second point, $(1,0)$, and third point, $(1,1)$, are the only active constraints.
Subtracting Equation \ref{eq:ineq2} from \ref{eq:ineq3} results in $-4c_x + 1 = 0$, or $c_x = 1/4$.
Plugging $c_x = 1/4$ into Equation \ref{eq:ineq2} and solving for the radius of the circle yields $r = \sqrt{13}/4$.
The values $c_x = 1/4$ and $r = \sqrt{13}/4$ satisfy Equations \ref{eq:ineq1}--\ref{eq:ineq4} as desired.
Since $\mu_1 = \mu_4 = 0$, Equations \ref{eq:kkt1} and \ref{eq:kkt2} become: 

\begin{eqnarray*}
	\mu_2 + \mu_3 &=& 1,\\
	5/2 - 2\mu_2 - 6\mu_3 &=& 0.
\end{eqnarray*}

Solving the system of equations yields $\mu_2 = 7/8$ and $\mu_3 = 1/8$.

The values $\mu_1 = \mu_4 = 0$, $\mu_2 = 7/8$, $\mu_3 = 1/8$, $c_x = 1/4$, and $r = \sqrt{13}/4$ satisfy the KKT conditions.

%Intuition might lead us to consider the constraints imposed by the third and fourth points because the distance between these two points is $\sqrt{2}$ and is greater than or equal to the distance between any other pair of points.
%Subtracting Equation \ref{eq:ineq4} from \ref{eq:ineq3} results in $-6c_x + 2 = 0$, or $c_x = 1/3$.
%Plugging $c_x = 1/3$ into Equation \ref{eq:ineq3} and solving for the radius of the circle yields $r = \sqrt{5}/3$.
%The values $c_x = 1/3$ and $r = \sqrt{5}/3$ satisfy Equation \ref{eq:ineq1}, but not Equation \ref{eq:ineq2}.
%This implies the assumption that the constraints imposed by the first and third points are active constraints is not true.

%Lastly, the optimal solution can be found by considering the constraints imposed by the second and fourth points.
%Subtracting Equation \ref{eq:ineq4} from \ref{eq:ineq2} results in $-2c_x + 1 = 0$, or $c_x = 1/2$.
%Plugging $c_x = 1/2$ into Equation \ref{eq:ineq4} and solving for the radius of the circle yields $r = \sqrt{5}/2$.
%The values $c_x = 1/2$ and $r = \sqrt{5}/2$ satisfy Equations \ref{eq:ineq1}--\ref{eq:ineq4} as desired.
%Since $\mu_1 = \mu_3 = 0$, Equation \ref{eq:kkt2} becomes $5 - 2\mu_2 = 0$ or $\mu_2 = 5/2$.

\item Interpret the meaning of the Lagrange multipliers.

The magnitude of the Lagrange multipliers represent the relative amount of constraint that each active constraint enforces on the optimal solution, i.e. because $\mu_2 = 7/8 > 1/8 = \mu_3$, if the second point is removed (or no longer requires being enclosed) the optimal solution is less than, or more optimal than, if the third point were to be removed.

This was verified as follows.
If the second point is removed, the constraints imposed by the third and fourth points are active.
Algebra yields $c_x = 1/3$, $r = \sqrt{5}/3$, $\mu_3 = 5/9$, and $\mu_4 = 4/9$.
In the case of the third point being removed, the constraint imposed by the second point is the only active constraint.
Algebra yields $c_x = 1/5$, $r = 2\sqrt{5}/5$, $\mu_2 = 1$.
As the circle is larger in the case of the third point being removed, $2\sqrt{5}/5 > \sqrt{5}/3$, this shows that the second point puts more constraint on the optimal solution than the third point does.

\item Is the solution unique?

Yes.
The optimal solution is unique as the cost function is convex on $y = 2x$ (and no other combination of active/inactive constraints satisfies the KKT conditions).
However, there are an infinite number of feasible solutions, as a circle with a radius approaching infinity will enclose any given set of points.

\end{enumerate}

\subsection{Approach 2}

\begin{enumerate}[a)]
	\item Express the problem as an unconstrained optimization problem using the log barrier method.
	
	\begin{equation*}
	\min_{r,c_x} r^2 - \mu \sum_{i=1}^n \log\left[-(x_i - c_x)^2 - (y_i - 2c_x)^2 
	+ r^2\right]
	\end{equation*}
	
	\item Find the optimality conditions.
	
	The first order necessary condition is given by
	
	\begin{equation*}
	\left(\begin{bmatrix}2r \\ 0\end{bmatrix} - \mu \sum_{i=1}^n \left(\frac{1}{-(x_i - c_x)^2 - (y_i - 2c_x)^2 + r^2} \begin{bmatrix}2r \\ 2x_i + 4y_i - 10c_x\end{bmatrix}\right)\right)\mathbf{d} \ge \mathbf{0}, \forall \mathbf{d},
	\end{equation*}
	
	or,
	
	\begin{eqnarray*}
	\left(2r - \mu \left[\frac{2r}{-5c_x^2 + 4c_x - 1 + r^2} + \frac{2r}{-5c_x^2 + 2c_x - 1 +r^2} + \frac{2r}{-5c_x^2 + 6c_x - 2 +r^2} + \frac{2r}{-5c_x^2 +r^2}\right]\right)d_1 &\ge& 0,\\
	-\mu \left(\frac{4-10c_x}{-5c_x^2 + 4c_x - 1 + r^2} + \frac{2-10c_x}{-5c_x^2 + 2c_x - 1 +r^2} + \frac{6-10c_x}{-5c_x^2 + 6c_x - 2 +r^2} + \frac{-10c_x}{-5c_x^2 +r^2}\right)d_2 &\ge& 0.
	\end{eqnarray*}
	
	The second order necessary condition is that,
	
%	\begin{eqnarray*}
%		\mathbf{F(\mathbf{x})} = &\\
%	&2 - 2r^2\mu \left[\frac{1}{(-5c_x^2 + 4c_x - 1 + r^2)^2} + \frac{1}{(-5c_x^2 + 2c_x - 1 +r^2)^2} + \frac{1}{(-5c_x^2 + 6c_x - 2 +r^2)^2} + \frac{1}{(-5c_x^2 +r^2)^2}\right]
%	\mathbf{e}_1\mathbf{e}_1 + \\
%%	&2r\mu \left[\frac{10c_x - 4}{(-5c_x^2 + 4c_x - 1 + r^2)^2} + \frac{10c_x - 2}{(-5c_x^2 + 2c_x - 1 +r^2)^2} + \frac{10c_x -6}{(-5c_x^2 + 6c_x - 2 +r^2)^2} + \frac{10c_x}{(-5c_x^2 +r^2)^2}\right] \mathbf{e}_2\mathbf{e}_1 + \\
%	&\mu \left[\frac{4-10c_x}{-5c_x^2 + 4c_x - 1 + r^2} + \frac{2-10c_x}{-5c_x^2 + 2c_x - 1 +r^2} + \frac{6-10c_x}{-5c_x^2 + 6c_x - 2 +r^2} + \frac{-10c_x}{-5c_x^2 +r^2} \right] \mathbf{e}_2\mathbf{e}_2
%	\end{eqnarray*}

\begin{eqnarray*}
\mathbf{F(\mathbf{x})} = &\\
&\left[ 2r - \mu \sum_{i=1}^n \frac{2\left[-(x_i-c_x)^2-(y_i-2c_x)^2+r^2\right] - 4r^2}{\left[-(x_i - c_x)^2 - (y_i - 2c_x)^2 + r^2\right]^2} \right]\mathbf{e}_1\mathbf{e}_1 + \\
&\left[ - \mu \sum_{i=1}^n \frac{2\left[2(x_i-c_x)+4(y_i-2c_x)\right]}{\left[-(x_i - c_x)^2 - (y_i - 2c_x)^2 + r^2\right]^2} \right]\mathbf{e}_1\mathbf{e}_2 + \\
&\left[ - \mu \sum_{i=1}^n \frac{2\left[2(x_i-c_x)+4(y_i-2c_x)\right]}{\left[-(x_i - c_x)^2 - (y_i - 2c_x)^2 + r^2\right]^2} \right]\mathbf{e}_2\mathbf{e}_1 + \\
&\left[ - \mu \sum_{i=1}^n \frac{-10\left[-(x_i - c_x)^2 - (y_i - 2c_x)^2 + r^2\right] - \left[2(x_i-c_x)+4(y_i-2c_x)\right]^2}{\left[-(x_i - c_x)^2 - (y_i - 2c_x)^2 + r^2\right]^2} \right]\mathbf{e}_2\mathbf{e}_2,
\end{eqnarray*}

be a positive semi-definite matrix.
	
	\item Describe a strategy to produce a feasible initial estimate for this method.
	
	One easy way to produce a feasible initial estimate would be to pick $c_x$ arbitrarily (say $c_x = c_y = 0$) then let $r = \max_i\left[(x_i-c_x)^2 + (y_i-2c_x)^2\right]$.
	This will give a circle with an arbitrary center that encloses all of the points.
	
	\item Do the values calculated in Approach 1 satisfy the optimality conditions in Approach 2?
	
	No.
	The issue with the log-barrier method is that the cost (and gradient of the cost) approach $\mu \cdot \infty$ at the constraints.
	This means the only way that this log-barrier method can approach the optimal solution is  if $\mu \rightarrow 0$.
	
\end{enumerate}

\newpage

\section{Problem 2}

Consider the optimization problem:

\begin{eqnarray*}
\min & f(\mathbf{x}), \\
\text{subject to} & h(\mathbf{x}) = 0,
\end{eqnarray*}

has a solution $\mathbf{x}^*$.
Let M be an optimistic estimate of $f(\mathbf{x}^*)$, that is $M \le f(\mathbf{x}^*)$.
Define a new cost function
\begin{equation} \label{eq:pencost}
v(M,\mathbf{x}) = \left[f(\mathbf{x})-M\right]^2 + [h(\mathbf{x})]^2,
\end{equation}
and the unconstrained problem of finding $\mathbf{x}$ to minimize the cost function.
For a given value of $M$, a solution $x_M$ to the unconstrained problem is found, and then M is updated through
\begin{equation} \label{eq:penupdate}
M_{k+1} = M_k + \sqrt{v(M_k,\mathbf{x}_{M_k})},
\end{equation}
and the process is repeated until convergence is reached.

\begin{enumerate}
	\item \label{step:opt} Show that if $M=f(\mathbf{x}^*)$, a solution to the unconstrained problem is a solution to the original problem, i.e., $\mathbf{x}_M = \mathbf{x}^*$.
	
	The cost function given by Equation \ref{eq:pencost} is the sum of two non-negative terms.
	To minimize Equation \ref{eq:pencost}, we need to minimize the absolute value of each quantity being squared.
	The first quantity, $f(\mathbf{x}) - M$, becomes $f(\mathbf{x}) - f(\mathbf{x}^*)$, assuming $M = f(\mathbf{x}^*)$.
	The absolute value of $f(\mathbf{x}) - f(\mathbf{x}^*)$ becomes zero when $\mathbf{x}_M = \mathbf{x}^*$.
	This is also true of the absolute value of the other term, $h(\mathbf{x})$.
	We know that $h(\mathbf{x}^*)$ is equal to zero because $\mathbf{x}^*$ is a feasible solution to the original constrained problem.
	This means we have shown not only that if $M=f(\mathbf{x}^*)$ then $\mathbf{x}_M = \mathbf{x}^*$, but also that the cost given by Equation \ref{eq:pencost} is zero when this condition is met.
	
	\item \label{step:ineq} Show that if $\mathbf{x}_M$ is a solution to the unconstrained problem, then $f(\mathbf{x}_M) \le f(\mathbf{x}^*)$.
	
	If $\mathbf{x}_M$ is a solution to the unconstrained optimization problem then we know $v(M, \mathbf{x}_M) \le v(M, \mathbf{x}_M)$ for all $\mathbf{x}$, including $\mathbf{x}^*$.
	This gives us the following:
	\begin{eqnarray*}
		v(M, \mathbf{x}_M) &\le& v(M, \mathbf{x}^*),\\
		\left[f(\mathbf{x}_M)-M\right]^2 + \left[h(\mathbf{x}_M)\right]^2 &\le& \left[f(\mathbf{x}^*)-M\right]^2, \\
		\left[f(\mathbf{x}_M)-M\right]^2 &\le& \left[f(\mathbf{x}^*)-M\right]^2, \\
		f(\mathbf{x}_M) &\le& f(\mathbf{x}^*),
	\end{eqnarray*}
	
	as desired.
	
	\item \label{step:ineq2} Show that if $M_k \le f(\mathbf{x}^*)$, then $M_{k+1} \le f(\mathbf{x}^*)$ when using the update equation above.
	
	%If $M_k \le f(\mathbf{x}^*)$ then $M_{k+1} - \sqrt{v(M_k,\mathbf{x}_{M_k})} \le f(\mathbf{x}^*)$.
	%Because $\mathbf{x}_{M_k}$ is a solution to the unconstrained optimization problem we know $v(M, \mathbf{x}_{M_k}) \le v(M, \mathbf{x}^*)$ which implies $M_{k+1} - \sqrt{v(M_k,\mathbf{x}^*)} \le M_{k+1} - \sqrt{v(M_k,\mathbf{x}_{M_k})} \le f(\mathbf{x}^*)$.
	%Expanding this inequality using Equation \ref{eq:pencost} yields:
	%\begin{eqnarray*}
	%	M_{k+1} - \sqrt{v(M_k,\mathbf{x}^*)} &\le& f(\mathbf{x}^*), \\
	%	M_{k+1} - \sqrt{\left[f(\mathbf{x}^*)-M_k\right]^2} &\le& f(\mathbf{x}^*), \\
	%	M_{k+1} - f(\mathbf{x}^*)+M_k &\le& f(\mathbf{x}^*), \\
	%\end{eqnarray*}
	%but $M_k \le f(\mathbf{x}^*)$ so,
	%\begin{eqnarray*}
	%	M_{k+1} - \sqrt{v(M_k,\mathbf{x}^*)} &\le& f(\mathbf{x}^*), \\
	%	M_{k+1} - \sqrt{\left[f(\mathbf{x}^*)-M_k\right]^2} &\le& f(\mathbf{x}^*), \\
	%	M_{k+1} - f(\mathbf{x}^*)+M_k &\le& f(\mathbf{x}^*), \\
	%\end{eqnarray*}
	
	The update equation gives us the relationship $M_{k+1} = M_k + \sqrt{v(M_k,\mathbf{x}_{M_k})}$. 
	Because $v(M_k,\mathbf{x}_{M_k}) \le v(M_k,\mathbf{x}^*)$, we know $M_{k+1} \le M_k + \sqrt{v(M_k,\mathbf{x}^*})$.
	Some algebra yields,
	
	\begin{eqnarray*}
		M_{k+1} &\le& M_k + \sqrt{v(M_k,\mathbf{x}^*)},\\
		M_{k+1} &\le& M_k + \sqrt{\left[f(\mathbf{x}^*)-M_k\right]^2}.
	\end{eqnarray*}
	
	If $M_k = f(\mathbf{x}^*)$ then $M_{k+1} = M_k = f(\mathbf{x}^*)$.
	If $M_k < f(\mathbf{x}^*)$ then $M_{k+1} < M_k - f(\mathbf{x}^*) - M_k = f(\mathbf{x}^*)$.
	This gives us the desired inequality, that is, $M_{k+1} \le f(\mathbf{x}^*)$.
	
	\item Show that $M_k \rightarrow f(\mathbf{x}^*)$ as $k$ increases.
	
	Upon inspection of the update function, it is clear that $M_k$ is monotonically increasing as $k$ increases because $\sqrt{v(M_k,\mathbf{x}_{M_k})}$ is non-negative for all values of $M_k$ and $\mathbf{x}_{M_k}$.
	We also know from Part \ref{step:ineq2} that if $M_1 \le f(\mathbf{x}^*)$, then each successive iteration $M_2, M_3, ...$ will also be bounded by $f(\mathbf{x}^*)$ from above.
	The sequence $M_k \rightarrow f(\mathbf{x}^*)$ as $k \rightarrow \infty$ because $M_k$ is monotonically increasing, it is bounded by $f(\mathbf{x}^*)$ from above, and $v(M,\mathbf{x}) \ge 0$ will only equal zero (effectively stopping $M_k$ from continuing to increase) when $\mathbf{x}_{M_k}$ is optimal (refer to Part 3) and feasible (otherwise $[h(\mathbf{x})]^2 > 0$).
	
	\item What does this method produce if your initial value for M is not optimistic, i.e. $M_1 > f(\mathbf{x}^*)$?
	
	The sequence $M_k$ is still monotonically increasing but is no longer bounded.
	This means that $M_k \rightarrow \infty$ as $k \rightarrow \infty$, i.e., the solution diverges.
	
	\item Describe a simple strategy to produce an optimistic initial value for $M$, (other than $M = -\infty$).
	
	Use a simple unconstrained optimization method (method of steepest descent, Newton's method, etc.) to find a minimum of $f(\mathbf{x})$ without any constraints.
	This value should be less than or equal to the same problem with constraints imposed, making it an optimistic estimate.
	
\end{enumerate}

\newpage

\section{Problem 3}

Consider the constrained optimization problem:

\begin{eqnarray*}
	\min & \mathbf{c}^T \mathbf{x}, \\
	\text{subject to} & \mathbf{a}^T \mathbf{x} &\le 1, \\
	& \mathbf{x} &\le \mathbf{0}.
\end{eqnarray*}

\begin{enumerate}

\item Write the KKT optimization conditions. How many equations and how many unknowns do you have?

\begin{eqnarray}
	\boldsymbol{\nabla}\mathbf{c}^T \mathbf{x} + \mu_1 \boldsymbol{\nabla} (\mathbf{a}^T \mathbf{x} - 1) + \sum_{i=1}^n \lambda_i \boldsymbol{\nabla} x_i &=& 0, \\
	\label{eq:kkt31} \mathbf{c} + \mu_1 \mathbf{a} + \boldsymbol{\lambda} &=& 0,
\end{eqnarray}

where $\boldsymbol{\lambda} = \begin{bmatrix}
\lambda_1 & \lambda_2 & ... & \lambda_{n}
\end{bmatrix}^T$ is an $n$ element vector of Lagrange multipliers, and,

\begin{eqnarray}
	\label{eq:kkt32} \mu_1(\mathbf{a}^T \mathbf{x} - 1) &=& 0,\\
	\label{eq:kkt33} \forall i, \lambda_i x_i &=& 0.
\end{eqnarray}

Equation \ref{eq:kkt31} represents a system of $n$ equations and $n+1$ unknowns.
Equation \ref{eq:kkt32} is one equation with $n$ additional unknowns, $\mathbf{x}$.
Equation \ref{eq:kkt33} is a system of $n$ equations with no additional unknowns.
This gives us a total of $2n + 1$ equations and $2n + 1$ unknowns.

\item Barrier method.
\begin{enumerate}[a)]
	\item Rewrite the problem as an unconstrained optimization problem using the log barrier function.
	
	\begin{equation*}
	B_\mu(\mathbf{x}) = \mathbf{c}^T \mathbf{x} - \mu\left[\log(1 - \mathbf{a}^T \mathbf{x}) + \sum_{i=1}^n \log(-x_i)\right]
	\end{equation*}
	
	\item Derive the first order necessary conditions for the unconstrained optimization problem.
	
	\begin{eqnarray*}
	\boldsymbol{\nabla}\left(\mathbf{c}^T \mathbf{x} - \mu\left[\log(1 - \mathbf{a}^T \mathbf{x}) + \sum_{i=1}^n \log(-x_i)\right]\right)\mathbf{d} &\ge& 0, \\
	\left(\mathbf{c} - \mu\left[\frac{-\mathbf{a}}{1 - \mathbf{a} \mathbf{x}} + \begin{bmatrix} \frac{1}{x_1} \\ \frac{1}{x_2} \\ \vdots \\ \frac{1}{x_n}\end{bmatrix}^T\right]\right)\mathbf{d} &\ge& 0, \\
	\left(\mathbf{c} - \mu\left[\frac{-\mathbf{a}}{1 - \mathbf{a}^T \mathbf{x}} + \sum_{i=1}^n \frac{\mathbf{e}_i}{x_i}\right]\right)\mathbf{d} &\ge& 0.
	\end{eqnarray*}
	
	\item Derive the Hessian of the unconstrained problem.
	
	\begin{eqnarray*}
		\mathbf{F}(\mathbf{x}) &=& \boldsymbol{\nabla}\left(\mathbf{c} - \mu\left[\frac{-\mathbf{a}}{1 - \mathbf{a}^T \mathbf{x}} + \sum_{i=1}^n \frac{\mathbf{e}_i}{x_i}\right]\right), \\
		&=& -\boldsymbol{\nabla}\mu\left[\frac{-\mathbf{a}}{1 - \mathbf{a}^T \mathbf{x}} + \sum_{i=1}^n \frac{\mathbf{e}_i}{x_i}\right], \\
		&=& -\mu\left[\boldsymbol{\nabla}\frac{-\mathbf{a}}{1 - \mathbf{a}^T \mathbf{x}} + \sum_{i=1}^n \boldsymbol{\nabla}\frac{\mathbf{e}_i}{x_i}\right],\\
		&=& -\mu\left[\frac{-\mathbf{a}\mathbf{a}^T}{(1 - \mathbf{a}^T \mathbf{x})^2} - \sum_{i=1}^n \frac{\mathbf{e}_i\mathbf{e}_i^T}{x_i^2}\right], \\
		&=& \mu\left[\frac{1}{(1 - \mathbf{a}^T \mathbf{x})^2}\begin{bmatrix}
			a_1^2 & a_1 a_2 & ... & a_1 a_n \\
			a_2 a_1 & a_2^2 & ... & a_2 a_n \\
			\vdots & \vdots & ... & \vdots \\
			a_n a_1 & a_n a_2 & ... & a_n^2 \\
		\end{bmatrix} + \begin{bmatrix}
		\frac{1}{x_1^2} & 0 & ... & 0 \\
		0 & \frac{1}{x_2^2} & ... & 0 \\
		\vdots & \vdots & ... & \vdots \\
		0 & 0 & ... & \frac{1}{x_n^2} \\
	\end{bmatrix}\right].
	\end{eqnarray*}
	
	The positive semi-definiteness of the Hessian is dependent on $\mathbf{a}$ and $\mathbf{x}$.
	From the above formulation, it is clear that the values of the Hessian on the diagonal will always be positive.
	For the Hessian to be positive semi-definite it is sufficient that all of its principal minors all be greater than or equal to zero.
	This can guaranteed if the off diagonal terms are less than or equal to zero.
	
\end{enumerate}
	
\end{enumerate}

\newpage

\section{Problem 4}

Given a positive-definite matrix $\mathbf{A}$, you are tasked with finding its largest and smallest eigenvalues and corresponding eigenvectors.

\begin{enumerate}
	\item Show that the constrained optimization problem,
	\begin{eqnarray*}
		\min & \mathbf{x}^T\mathbf{A}\mathbf{x}, \\
		\text{subject to} & \mathbf{x}^T\mathbf{x} = 1,
	\end{eqnarray*}
	has a solution $\mathbf{x}^*$ that is the eigenvector corresponding to the smallest eigenvalue.
	
	If $\mathbf{A}$ is a symmetric matrix and $\mathbf{D}$ is a diagonal matrix whose diagonal entries are the eigenvalues of $\mathbf{A}$, then there exists an orthogonal matrix $\mathbf{Q}$ such that $\mathbf{A} = \mathbf{Q}^{-1} \mathbf{D} \mathbf{Q}$.
	The matrix $\mathbf{A}$ is given as positive definite so we know that it is a symmetric matrix.
	Orthogonal matrices have interesting properties, namely $Q^T = Q^{-1}$, and as a linear transformation, an orthogonal matrix preserves the dot product of two vectors.
	
	Let $\mathbf{u}$ be such a linear transformation so that $\mathbf{u} = \mathbf{Q}^T\mathbf{x}$.
	Note that by the properties of an orthogonal matrix previously described, $\mathbf{x}^T\mathbf{A}\mathbf{x} = \mathbf{x}^T\mathbf{Q}^T \mathbf{D} \mathbf{Q}\mathbf{x} = \mathbf{u}^T\mathbf{D}\mathbf{u}$ and $\mathbf{x}^T\mathbf{x} = \mathbf{u}^T\mathbf{u}$.
	Using these transformations we can rewrite the constrained optimization problem as,
	\begin{eqnarray*}
		\min & \mathbf{u}^T\mathbf{D}\mathbf{u}, \\
		\text{subject to} & \mathbf{u}^T\mathbf{u} = 1.
	\end{eqnarray*}
	
	The constraint $\mathbf{u}^T\mathbf{u} = 1$ enforces that $\mathbf{u}$ be a unit vector.
	The cost function can be rewritten as $D_{11} u_1^2 + D_{22} u_2^2 + ... + D_{nn} u_n^2$ because $\mathbf{D}$ is a diagonal matrix.
	Some thought shows us that the optimal solution results in $u_i = 1$, $u_j = 0, \forall j \ne i$, where $D_{ii}$ is the smallest eigenvalue of the matrix $\mathbf{A}$.
	%Using the linear transformation, $\mathbf{Q}$, to map $\mathbf{u}$ back to the original space results in $\mathbf{x} = \mathbf{Q}\mathbf{u}$ which is the eigenvector corresponding to the smallest eigenvalue of $\mathbf{A}$.
	Lastly, because $\mathbf{A}$ is positive definite, the $i^{th}$ column of $\mathbf{Q}$ corresponds to the $i^{th}$ eigenvalue in the diagonal matrix $\mathbf{D}$.
	Since $\mathbf{x} = \mathbf{Q} \mathbf{u}$, the solution to the optimization problem, $\mathbf{x}^*$, is the eigenvector corresponding to the smallest eigenvalue of $\mathbf{A}$.
	
	\item What is the cost value at the minimum in relation to the smallest eigenvalue?
	
	It was shown previously that at the minimum $\mathbf{u}^T\mathbf{D}\mathbf{u}$ is equal to the smallest eigenvalue.
	Since $\mathbf{u}^T\mathbf{D}\mathbf{u} = \mathbf{x}^T\mathbf{A}\mathbf{x}$, the cost of the original constrained optimization problem is also equal to the smallest eigenvalue.
	
	\item Express finding the largest eigenvalue and corresponding eigenvector as an optimization problem similar to the one above.
	
	To find the largest eigenvalue and corresponding eigenvector,
	
	\begin{eqnarray*}
		\max & \mathbf{x}^T\mathbf{A}\mathbf{x}, \\
		\text{subject to} & \mathbf{x}^T\mathbf{x} = 1,
	\end{eqnarray*}
	
	or equivalently,
	
	\begin{eqnarray*}
		\min & -\mathbf{x}^T\mathbf{A}\mathbf{x}, \\
		\text{subject to} & \mathbf{x}^T\mathbf{x} = 1.
	\end{eqnarray*}
	
	\item Exact Penalty Method
	
	\begin{enumerate}[a)]
		\item Express the constrained minimization in part 1 as an unconstrained optimization problem using the exact penalty method with an absolute value penalty term and penalty parameter $c$.
		
		The optimization problem rewritten as unconstrained and using an absolute value penalty term is given by,
		
		\begin{eqnarray}
		\label{eq:expen}
			\min & \mathbf{x}^T\mathbf{A}\mathbf{x} - c|\mathbf{x}^T\mathbf{x} - 1|. 
		\end{eqnarray}
		
		\item For what values of the penalty parameter $c$ is the optimal $\mathbf{x}_c = \mathbf{x}^*$?
		
		The first order condition for the optimal solution of Equation \ref{eq:expen} is,
		
		\begin{eqnarray*}
		\boldsymbol{\nabla}\left(\mathbf{x}^T\mathbf{A}\mathbf{x} - c|\mathbf{x}^T\mathbf{x} - 1|\right)\mathbf{d} &\ge& 0,\\
		\left(2\mathbf{x}^T\mathbf{A} - c\boldsymbol{\nabla}|\mathbf{x}^T\mathbf{x} - 1|\right)\mathbf{d} &\ge& 0,\\
		\left(2\mathbf{x}^T\mathbf{A} - 2c\mathbf{x}\frac{\mathbf{x}^T\mathbf{x} - 1}{|\mathbf{x}^T\mathbf{x} - 1|}\right)\mathbf{d} &\ge& 0.
		\end{eqnarray*}
		
		The penalty parameter $c$ must satisfy the inequality above for $\mathbf{x}_c = \mathbf{x}^*$ to be optimal.
		
	\end{enumerate}
	
\end{enumerate}

\end{document}